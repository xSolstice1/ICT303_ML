{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mKK4Rd8vhPS"
      },
      "source": [
        "#  **ICT303 - Assignment 1**\n",
        "\n",
        "**Your name: Ang Jin Wei**\n",
        "\n",
        "**Student ID: 34792195**\n",
        "\n",
        "**Email: xsolsticegfx@gmail.com**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Description**\n",
        "\n",
        "We would like to develop, using Multilayer Perceptron (MLP), a computer program that takes images of handwritten text, finds the written characters in the image and displays the written characters.\n",
        "\n",
        "To achieve this, we will proceed in steps:\n",
        "\n",
        "1. Develop and train an MLP for the recognition of handwritten characters from images. In the first instance, the images are assumed to contain only one handwritten.\n",
        "2. Train and test the MLP, and evaluate its performance by using loss curves and proper accuracy/performance measures\n",
        "3. Improve the performance of the MLP by tuning its hyper parameters.\n",
        "4. Extend the program you developed to localize (detect) and recognize handwritten characters in an image that contains multiple handwritten characters.\n",
        "\n",
        "For this purpose, we will use the following dataset for training, validation and testing: https://www.kaggle.com/datasets/dhruvildave/english-handwritten-characters-dataset.\n",
        "\n",
        "You are required to justify every design choice. Justifications should be theoretical and validated with experiments.\n",
        "\n",
        "It is important that you start as earlier as possible. Coding is usually easy. However, training neural networks and tuning its hyper-parameters takes time."
      ],
      "metadata": {
        "id": "by2yigAYoub0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Marking Guide**##\n",
        "\n",
        "- The overal structure of the program - it should follow the structure we used so far in the labs **[30 Marks]**. This includes:\n",
        "  - A class that defines the network architecture that extends the class `nn.Module`. It should have a constructor method (`__init__()`) and a forward function (`forward()`)\n",
        "  - The Trainer class\n",
        "  - A main function\n",
        "\n",
        "- Training working and running on GPU **[10 marks]**\n",
        "\n",
        "- Curves for training loss and validation loss plotted and training stopped when the network starts to overfit (i.e., when the validation loss starts to increase). You must use TensorBoard to visualize curves and monitor performance **[10 marks]**\n",
        "\n",
        "- Testing code properly working. **[10 marks]**\n",
        "\n",
        "- Hyper parameters finetuned and the best ones selected. **[10 marks]**\n",
        "\n",
        "- Quality of the dicussions **[20 marks]**: did the student discuss various design choices, including the hyperparamters or any choices they made to improve the performance? Any design choice should be properly justified.\n",
        "\n",
        "- Extension to the localization of the characters **[10 marks]**"
      ],
      "metadata": {
        "id": "y9y_vBKmcS3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. What to submit**\n",
        "\n",
        "You need to upload to LMS the notebook as well as a folder that contains the .py files you created. All classes should be implemented in .py files. The notebook will sever as a documentation of your work as well as the codes that demonstrated the training, validation and testing of your MLP models that you created.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPY9Ha_3qs68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "t2HPHmxTFHKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "A computer program using Multilayer Perceptron (MLP) that takes images of handwritten text, finds the character written in the image and display the written characters.\n",
        "\n",
        "Notes:\n",
        "1. This program is coded in Visual Studio Code using locally downloaded datasets. The .py files and datasets will be zipped and uploaded in the LMS.\n",
        "2. I am running the program on Nvidia GTX 3080 graphics card.\n",
        "3. The program is **not made to work** inside Colab notebook as it requires local dataset, hence, please run the program through the zipped file along with the datasets included, in order to run it in Colab notebook, you will have to manually upload the local dataset to the notebook following the same folder/data structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "zX7g3as6FP2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Organizing of data files"
      ],
      "metadata": {
        "id": "zGoCxjaeOlWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Read CSV file (assuming it has columns 'Image' and 'Class')\n",
        "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
        "dataset_folder = 'Data/lowercase.csv'\n",
        "image_folder = 'Data/Img'\n",
        "csv_path = os.path.join(dir_path, dataset_folder)\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Path to the folder containing all images\n",
        "image_folder = os.path.join(dir_path, image_folder)\n",
        "\n",
        "# Loop through each row in the CSV file\n",
        "for index, row in df.iterrows():\n",
        "    image_filename = row['image']\n",
        "    class_label = str(row['label'])  # Ensure class labels are converted to strings\n",
        "\n",
        "    # Create a folder for the class if it doesn't exist\n",
        "    class_folder = os.path.join(image_folder, class_label)\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "    # Move the image to the class folder\n",
        "    source_path = os.path.join(image_folder, image_filename)\n",
        "    destination_path = os.path.join(class_folder, image_filename)\n",
        "\n",
        "    # Check if the file exists before moving\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.move(source_path, destination_path)\n",
        "    else:\n",
        "        print(f\"File {image_filename} not found.\")\n",
        "\n",
        "print(\"Images organized into folders based on class labels.\")"
      ],
      "metadata": {
        "id": "jiR21N31O93p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data files are organized using the above program.\n",
        "\n",
        "1. I will separate the csv files into 2 (lowercase.csv and english.csv)\n",
        "2. The data in the csv will be renamed from Img/ImgName.png to just ImgName.png\n",
        "3. I will run the program on both the csv files\n",
        "4. The data images will be moved to their respective class folders for easier importation into the training model later."
      ],
      "metadata": {
        "id": "FYT8d6HePAcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Importing dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "KULPjb43HqRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "nEqAS_E6IEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the following dependencies to be used in the program.\n",
        "\n",
        "os: The os module provides a way to interact with the operating system, and it is used here to handle file paths and directory operations.\n",
        "\n",
        "torch: This is the core PyTorch library, which provides tensor computation functionalities. PyTorch is commonly used for building and training neural network models.\n",
        "\n",
        "nn (torch.nn): The nn module in PyTorch provides classes for building and training neural networks. It includes various neural network layers, loss functions, and other utilities.\n",
        "\n",
        "tqdm: tqdm is a library for adding progress bars to loops. In this code, it is used to visualize the progress of training epochs using the tqdm function.\n",
        "\n",
        "DataLoader (torch.utils.data.DataLoader): The DataLoader class is part of PyTorch's torch.utils.data module, and it provides an efficient way to load and iterate over datasets during training.\n",
        "\n",
        "transforms (torchvision.transforms): The transforms module from torchvision is used for defining and applying various image transformations. In this code, it is used to compose a set of transformations on the input images.\n",
        "\n",
        "torchvision: This is a PyTorch library specifically designed for computer vision tasks. It includes datasets, models, and utilities for working with image data.\n",
        "\n",
        "ImageFolder (torchvision.datasets.ImageFolder): ImageFolder is a dataset class from torchvision.datasets that simplifies the loading of image datasets by assuming a specific directory structure.\n",
        "\n",
        "SummaryWriter (torch.utils.tensorboard.SummaryWriter): The SummaryWriter class is part of PyTorch's TensorBoard integration, allowing for the logging of training metrics and visualizations that can be viewed in TensorBoard."
      ],
      "metadata": {
        "id": "VOSfMeyGIHNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model (MLP) Class"
      ],
      "metadata": {
        "id": "zSRpFmGZIVIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=224*224*3, output_size=62, lr=0.001):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_size, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Add dropout for regularization\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_size),\n",
        "        )\n",
        "\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #return torch.optim.SGD(self.parameters(), self.lr, momentum=0.9)\n",
        "        return torch.optim.Adam(self.parameters(), self.lr)\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "        fn = nn.CrossEntropyLoss()\n",
        "        return fn(y_hat, y)"
      ],
      "metadata": {
        "id": "ckTVzxD8IZU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The design of the MLP (Multi-Layer Perceptron) model is a choice commonly made for image classification tasks. Here's a brief description of the design choices:\n",
        "\n",
        "1. **Input Layer:**\n",
        "\n",
        "The model starts with a Flatten() layer, which is used to flatten the input image tensor. This is necessary when dealing with image data, as it transforms the multi-dimensional input into a flat vector.\n",
        "2. **Hidden Layers:**\n",
        "\n",
        "The model consists of three fully connected (linear) hidden layers with 1024, 512, and 256 neurons, respectively. These layers are responsible for learning hierarchical representations of the input data, capturing complex patterns and features.\n",
        "3. **Batch Normalization:**\n",
        "\n",
        "Batch normalization layers (nn.BatchNorm1d()) are inserted after each linear layer. Batch normalization helps stabilize and accelerate the training process by normalizing the input of each layer across mini-batches.\n",
        "4. **Activation Function:**\n",
        "\n",
        "ReLU (Rectified Linear Unit) activation functions (nn.ReLU()) are applied after each batch normalization layer. ReLU introduces non-linearity to the model, allowing it to learn complex relationships in the data.\n",
        "5. **Dropout:**\n",
        "\n",
        "Dropout layers (nn.Dropout()) are included after the first and second linear layers. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero during training. This helps improve the generalization of the model.\n",
        "6. **Output Layer:**\n",
        "\n",
        "The final linear layer produces the output with a size equal to the specified output_size. For classification tasks, a common practice is to use the softmax activation function on the output layer for obtaining probability distributions over different classes.\n",
        "7. **Loss Function:**\n",
        "\n",
        "The model uses the CrossEntropyLoss (nn.CrossEntropyLoss()) as the loss function. CrossEntropyLoss is suitable for multi-class classification problems, and it combines the softmax activation with the negative log likelihood loss.\n",
        "8. **Optimizer:**\n",
        "\n",
        "The model is configured to use the Adam optimizer (torch.optim.Adam()) during training. Adam is an adaptive learning rate optimization algorithm that is well-suited for a wide range of optimization problems.\n",
        "\n",
        "Overall, this MLP architecture is a standard choice for image classification due to its simplicity, effectiveness, and the ability to capture hierarchical features in the data. The use of batch normalization, dropout, and appropriate activation functions contributes to better training stability and generalization."
      ],
      "metadata": {
        "id": "zVgmoJz4InL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Trainer Class"
      ],
      "metadata": {
        "id": "bRUVJ8cpJIbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, n_epochs, patience=5, log_dir='logs'):\n",
        "        self.max_epochs = n_epochs\n",
        "        self.patience = patience\n",
        "        self.current_patience = 0\n",
        "        self.best_validation_loss = float('inf')\n",
        "        self.log_dir = log_dir\n",
        "        self.writer = SummaryWriter(log_dir=self.log_dir)  # Use self.log_dir\n",
        "        self.global_step = 0  # Initialize global_step attribute\n",
        "        self.validation_data = None\n",
        "\n",
        "    def fit(self, model, data):\n",
        "        self.data = data\n",
        "        self.optimizer = model.configure_optimizers()\n",
        "        self.model = model\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            current_loss = self.fit_epoch(epoch)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            validation_loss = self.calculate_validation_loss()\n",
        "\n",
        "            # Log validation loss\n",
        "            self.writer.add_scalar('validation_loss', validation_loss, len(self.data) * epoch)\n",
        "\n",
        "            # Check for early stopping\n",
        "            if validation_loss < self.best_validation_loss:\n",
        "                self.best_validation_loss = validation_loss\n",
        "                self.current_patience = 0\n",
        "            else:\n",
        "                self.current_patience += 1\n",
        "\n",
        "            if self.current_patience >= self.patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss.\")\n",
        "                break\n",
        "\n",
        "        print(\"Training process has finished\")\n",
        "\n",
        "    def fit_epoch(self, epoch):\n",
        "        current_loss = 0.0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(self.data, desc=f\"Epoch {epoch+1}/{self.max_epochs}\")):\n",
        "            inputs, target = data\n",
        "            inputs = inputs.to('cuda')\n",
        "            target = target.to('cuda')\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs.to(inputs.device))\n",
        "\n",
        "            loss = self.model.loss(outputs, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            current_loss += loss.item()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            #correct_prediction = torch.argmax(outputs, 1) == target\n",
        "            #correct_prediction = correct_prediction.sum()\n",
        "            #epoch_acc += correct_prediction\n",
        "\n",
        "            # Log training loss to TensorBoard\n",
        "            if i % 10 == 9:\n",
        "                self.writer.add_scalar('training_loss', current_loss / 10, len(self.data) * epoch + i)\n",
        "                current_loss = 0.0\n",
        "\n",
        "    def calculate_validation_loss(self):\n",
        "        self.model.eval()\n",
        "        validation_loss = 0.0\n",
        "        total_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, target in self.validation_data:\n",
        "                inputs, target = inputs.to('cuda'), target.to('cuda')\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.model.loss(outputs, target)\n",
        "                validation_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "        self.model.train()\n",
        "        return validation_loss / total_batches if total_batches > 0 else 0.0\n",
        "\n",
        "    def set_validation_data(self, validation_data):\n",
        "        self.validation_data = validation_data"
      ],
      "metadata": {
        "id": "dVFY4BXXJSM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialization:**\n",
        "\n",
        "The class is initialized with parameters such as the maximum number of epochs (n_epochs) and an optional directory for logging (log_dir). The default value for log_dir is set to 'logs'.\n",
        "It initializes the TensorBoard SummaryWriter (self.writer) to log training metrics for visualization.\n",
        "2. **Training Process (fit method):**\n",
        "\n",
        "The fit method is responsible for training the provided model (model) using the specified training data (data).\n",
        "It sets up the optimizer using the model's configure_optimizers method.\n",
        "It iterates over epochs and calls the fit_epoch method for each epoch.\n",
        "3. **Training Epoch (fit_epoch method):**\n",
        "\n",
        "The fit_epoch method handles the training for a single epoch. It iterates over the provided training data in batches.\n",
        "For each batch, it performs the forward and backward pass, updating the model's parameters.\n",
        "It calculates and logs the training loss and accuracy to TensorBoard every 10 batches.\n",
        "4. **Validation Loss Calculation (calculate_validation_loss method):**\n",
        "\n",
        "The calculate_validation_loss method evaluates the model on the validation data and returns the average validation loss.\n",
        "It sets the model to evaluation mode (self.model.eval()) to disable dropout and batch normalization during validation.\n",
        "The calculated validation loss is logged for monitoring the model's performance.\n",
        "5. **Validation Data Setup (set_validation_data method):**\n",
        "\n",
        "The set_validation_data method allows setting the validation data for later evaluation during training.\n",
        "6. **Device Usage:**\n",
        "\n",
        "The trainer assumes the use of a GPU ('cuda') for training and validation. It transfers input data and model to the GPU using .to('cuda').\n",
        "The model is set back to training mode after validation (self.model.train()) to re-enable dropout and batch normalization.\n",
        "7. **Logging to TensorBoard:**\n",
        "\n",
        "The training loss are logged to TensorBoard using the SummaryWriter (self.writer). This enables real-time monitoring of the training progress.\n",
        "\n",
        "The addition of TensorBoard logging further enhances the transparency of the training process, allowing for detailed monitoring of training and validation metrics. The design promotes modularity and flexibility, making it easy to adapt the trainer for various datasets and model architectures.\n",
        "\n",
        "8. **Early Stopping:**\n",
        "\n",
        "Early Stopping and Validation Patience:\n",
        "The Trainer class now incorporates an early stopping mechanism to prevent overfitting by monitoring the validation loss. The early stopping is implemented with a patience parameter (self.patience), which represents the number of epochs with no improvement in validation loss before stopping the training process.\n",
        "\n",
        "During each epoch, the Trainer calculates the validation loss using the calculate_validation_loss method. If the validation loss improves (i.e., decreases), the current patience counter (self.current_patience) is reset to zero. If there is no improvement, the counter is incremented.\n",
        "\n",
        "The early stopping condition is checked after each epoch. If the current patience exceeds the specified patience value, the training process is halted, and a message is printed indicating that early stopping occurred.\n",
        "\n",
        "This mechanism helps prevent the model from continuing to train when the validation loss ceases to improve, avoiding overfitting on the training data.\n",
        "\n",
        "## **Overall**\n",
        "\n",
        "The design choices aim to provide a flexible and organized structure for training neural network models, including features like validation, logging, and GPU acceleration. The use of TensorBoard facilitates visualizing key training metrics, aiding in model evaluation and tuning."
      ],
      "metadata": {
        "id": "GYrEHL2oJU7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Main Function"
      ],
      "metadata": {
        "id": "z5XNLyZUJ4vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set environ to use CUDA\n",
        "    os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "    # Set the path to your local path\n",
        "    current_directory = os.path.dirname(os.path.realpath(__file__))\n",
        "    dataset_folder = 'Data'\n",
        "    local_dataset_path = os.path.join(current_directory, dataset_folder)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((224, 224)) #900, 1200 => 224, 224\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.ImageFolder(root=local_dataset_path, transform=transform)\n",
        "    batch_size = 30\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    validation_dataset = torchvision.datasets.ImageFolder(root=local_dataset_path, transform=transform)\n",
        "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    print(train_dataset.classes)\n",
        "\n",
        "    # 2. The MLP model\n",
        "    mlp_model = MLP(lr=0.003)\n",
        "    mlp_model = mlp_model.to('cuda')\n",
        "    mlp_model.train()\n",
        "\n",
        "    # 3. Training the network\n",
        "    # 3.1. Creating the trainer class\n",
        "    trainer = Trainer(n_epochs=35)\n",
        "    trainer.set_validation_data(trainloader)\n",
        "\n",
        "    # 3.2. Training the model\n",
        "    trainer.fit(mlp_model, trainloader)\n",
        "\n",
        "    classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a_', 'b_', 'c_', 'd_', 'e_', 'f_', 'g_', 'h_', 'i_', 'j_', 'k_', 'l_', 'm_', 'n_', 'o_', 'p_', 'q_', 'r_', 's_', 't_', 'u_', 'v_', 'w_', 'x_', 'y_', 'z_']\n",
        "\n",
        "    testset = torchvision.datasets.ImageFolder(root=local_dataset_path, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    images = images.to('cuda')\n",
        "    labels = labels.to('cuda')\n",
        "\n",
        "    # calculate accuracy\n",
        "    def calculate_accuracy(model, dataloader, device='cuda'):\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_predictions += (predicted == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        model.train()  # Set the model back to training mode\n",
        "        return accuracy\n",
        "\n",
        "    #train_accuracy = calculate_accuracy(mlp_model, trainloader)\n",
        "    #print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "    test_accuracy = calculate_accuracy(mlp_model, testloader)\n",
        "    print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "    print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(images.shape[0])))\n",
        "\n",
        "    output = mlp_model(images)\n",
        "    estimated_labels = torch.max(output, 1).indices\n",
        "\n",
        "    print('Estimated Labels: ', ' '.join(f'{classes[estimated_labels[j]]:5s}' for j in range(images.shape[0])))"
      ],
      "metadata": {
        "id": "pV80EWf1KC51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main function in this code serves as the entry point for executing the entire script. Here's a short description of the design choices made in the main function:\n",
        "\n",
        "1. **CUDA Environment Configuration:**\n",
        "\n",
        "The script sets an environment variable ('TORCH_USE_CUDA_DSA') to enable CUDA (GPU) usage. This ensures that the code takes advantage of GPU acceleration if available.\n",
        "2. **Dataset Loading and Transformation:**\n",
        "\n",
        "The script defines a set of transformations using transforms.Compose from torchvision.transforms to preprocess the input images. These transformations include converting images to tensors and resizing them to a specific size (224x224).\n",
        "It creates instances of ImageFolder for the training, validation, and test datasets using the specified transformation.\n",
        "3. **Model Initialization:**\n",
        "\n",
        "An instance of the MLP model (mlp_model) is created with a specified learning rate (lr=0.003). The model is then moved to the GPU ('cuda') using the .to('cuda') method.\n",
        "4. **Trainer Initialization and Training:**\n",
        "\n",
        "An instance of the Trainer class (trainer) is created with a maximum number of epochs set to 25. The validation data is set to the training loader using the set_validation_data method.\n",
        "The model is trained using the fit method of the Trainer class, where the training loader is provided as the training data.\n",
        "5. **Accuracy Calculation on Test Set:**\n",
        "\n",
        "The script calculates the accuracy of the trained model on a separate test set using the calculate_accuracy function. The results are printed to the console.\n",
        "6. **Printing Ground Truth and Estimated Labels:**\n",
        "\n",
        "The ground truth labels and the labels estimated by the model are printed to the console for visual inspection.\n",
        "\n",
        "Overall, the main function orchestrates the entire training and evaluation process, from setting up the environment to training the model, calculating accuracy, and providing insights into the model's predictions on a test set. The design aims to be modular, readable, and organized, separating concerns into distinct sections for clarity."
      ],
      "metadata": {
        "id": "AZJeC_IUKWRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters & Weightage"
      ],
      "metadata": {
        "id": "0CYrsMSEKu9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this particular program, the parameters have been fine-tuned as follows:\n",
        "\n",
        "1. Learning Rate: 0.003\n",
        "2. Training Data Batch Size: 30\n",
        "3. Validation Data Batch Size: 30\n",
        "4. Test Data Batch Size: 100\n",
        "5. Number of Epochs: 35\n",
        "\n",
        "From the results of monitoring performance through TensorBoard, it was discovered that the least loss were observed for learning rates between 0.0001 to less than 1. Hence, Learning Rate is further adjusted to improve on the training speed and accuracy.\n",
        "\n",
        "Both the training data and validation data batch size is set to 30 as an effort to optimize the training speed while not jeopardizing the accuracy of the model.\n",
        "\n",
        "The test data batch size is set to 100 in order to test the model against a larger amount of data at once.\n",
        "\n",
        "Lastly, for the number of epochs, several tests were done with the same configuration above, here are the findings:\n",
        "\n",
        "1. 1 Epoch : Test Accuracy: 12.73%\n",
        "2. 5 Epochs : Test Accuracy: 45.54%\n",
        "3. 10 Epochs : Test Accuracy: 70.79%\n",
        "4. 20 Epochs : Test Accuracy: 85.51%\n",
        "5. 25 Epochs : Test Accuracy: 89.12%\n",
        "6. 30 Epochs : Test Accuracy: 92.52%\n",
        "7. 35 Epochs : Test Accuracy: 95.07%\n",
        "8. 40 Epochs : Test Accuracy: 93.99%\n",
        "\n",
        "Each Epoch takes roughly 35-40s to complete on my system.\n",
        "As we can see from the findings, every X increase on the number of epochs resulted in diminishing returns (Probably due to overfitting). With 35-40 Epoch resulting in a loss in accuracy. Hence, we can deduce 35 Epochs might be the optimal number of epochs to train the model.\n",
        "\n",
        "We can run it at 60 batch size for training and validation data, and at 35 Epoch and it resulted in a **Test Accuracy of 92.67%**, the loss in accuracy doesn't justify the faster speed of training brought by the increase batch size as the speed increase is minimal.\n",
        "\n",
        "When we try to run 35 Epochs on 0.03 Learning Rate instead of 0.003 Learning Rate, we end up with a poor accuracy of **1.67%**, hence, I believe that 0.003 Learning Rate would be the optimal Learning Rate for this particular model.\n",
        "\n",
        "Now to finetune the batch size of the training and validation model, we try running a lower number of epoch with a lower batch size.\n",
        "\n",
        "1. 16 Batch Size, 1 Epoch : Test Accuracy 12.61%\n",
        "2. 16 Batch Size, 5 Epochs : Test Accuracy 49.33%\n",
        "3. 16 Batch Size, 10 Epochs : Test Accuracy 66.28%\n",
        "4. 16 Batch Size, 20 Epochs : Test Accuracy 83.99%\n",
        "5. 16 Batch Size, 35 Epochs : Test Accuracy: 91.38%\n",
        "\n",
        "As we can see, having a lower batch size did not result in a better overall accuracy while slowing down the training time, hence from this we can conclude that **30 batch size** is ideal.\n",
        "\n",
        "It is also interesting to note that LeNet CNN model performs much better than MLP model, achieving 97% Test Accuracy with just 11 Epochs on 30 Batch Size.\n",
        "\n",
        "Final Parameters:\n",
        "\n",
        "1. Learning Rate: 0.003\n",
        "2. Training Data Batch Size: 30\n",
        "3. Validation Data Batch Size: 30\n",
        "4. Test Data Batch Size: 100\n",
        "5. Number of Epochs: 35\n"
      ],
      "metadata": {
        "id": "ao0_CrhnLNY2"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}